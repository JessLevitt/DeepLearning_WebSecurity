#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
This is a demo of malware detection, where mist dataset is used
"""

import re
import glob
import logging
import warnings
import numpy as np
from tensorflow import keras

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.neural_network import MLPClassifier

from xgboost import XGBClassifier

warnings.filterwarnings('ignore')
logging.getLogger().setLevel(logging.INFO)


def load_files_from_dir(dir_path):
    """Load files from directory

    Args:
        dir_path: string, the path of directory

    Returns:
        texts: list of string, store all texts of the files in this directory
    """

    # Get the paths of all files
    file_paths = glob.glob(dir_path)

    # Store all texts
    texts = []

    # Load all file texts
    for file_path in file_paths:

        with open(file_path, 'r', errors='ignore') as file:

            logging.info("Successfully load {}".format(file_path))
            text = file.read()

            # Replace malware label by ' '
            text = re.sub(r"APT1|Crypto|Locker|Zeus", ' ', text, flags=re.I)

            # Add current text to texts
            texts.append(text)

    return texts


def load_all_files():
    """Load all files

    Returns:
        all_texts: list of string, store all texts of files
        all_labels: list of int, store all labels corresponding to all_texts
    """

    # Store all texts from different directory
    all_texts = []
    all_labels = []

    # Define malware classes
    malware_class = ['APT1', 'Crypto', 'Locker', 'Zeus']

    # Get all texts and their corresponding labels
    for index, type in enumerate(malware_class):

        # Set current directory path
        dir_path = 'data/{}/*'.format(type)
        logging.info("Load files from {}".format(dir_path))

        # Load all texts from current directory
        texts = load_files_from_dir(dir_path)

        # Add to all texts list
        all_texts += texts

        # Construct corresponding labels
        all_labels += [index] * len(texts)
        logging.info("Load file form {} successfully".format(dir_path))

    return all_texts, all_labels


def get_tfidf_dataset(texts, labels, max_words):
    """Get tf-idf vectorized dataset

    Args:
        texts: list of string, storing all texts
        labels: list of int, storing all labels corresponding to texts
        max_words: int, the maximal number of words that are used

    Returns:
        X_train: array, shape (n_samples, max_words), the features of training dataset
        X_test: array, shape (n_samples, max_words), the features of testing dataset
        y_train, array, shape (n_samples), the labels of training dataset
        y_test, array, shape (n_samples), the labels of testing dataset
    """

    # Split texts and labels into train and test
    texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.3, shuffle=True)

    # Initialize a counter vectorizer
    tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 2), decode_error="ignore", lowercase=True, stop_words="english",
                                    max_features=max_words, binary=False, token_pattern=r'\b\w+\b')

    # Transform training texts as tf-idf vectors
    X_train = tfidf_vectorizer.fit_transform(texts_train).toarray()
    logging.info("Transform training text into tf-idf vector successfully")

    # Transform testing texts as tf-idf vectors
    X_test = tfidf_vectorizer.transform(texts_test).toarray()
    logging.info("Transform testing text into tf-idf vector successfully")

    # Get y of training dataset and testing dataset
    y_train = np.array(labels_train)
    y_test = np.array(labels_test)

    return X_train, X_test, y_train, y_test


if __name__ == "__main__":

    # Set the maximal number of words that are used
    max_words = 1000

    # Get all texts and labels
    all_texts, all_labels = load_all_files()

    # Get tf-idf vectorized dataset
    X_train, X_test, y_train, y_test = get_tfidf_dataset(all_texts, all_labels, max_words=max_words)

    # # Train and test SVM model on tf-idf dataset
    # svm_model = SVC(C=1.0, kernel='linear', decision_function_shape='ovr')
    # svm_model.fit(X_train, y_train)
    # y_test_predicted = svm_model.predict(X_test)
    # print("Accuracy score: \n", accuracy_score(y_test, y_test_predicted))       # 0.84
    # print("Clf report: \n", classification_report(y_test, y_test_predicted))
    # print("Confusion matrix: \n", confusion_matrix(y_test, y_test_predicted))

    # # Train and test Xgboost model on tf-idf dataset
    # xgb_model = XGBClassifier(n_estimators=100, n_jobs=-1)
    # xgb_model.fit(X_train, y_train)
    # y_test_predicted = xgb_model.predict(X_test)
    # print("Accuracy score: \n", accuracy_score(y_test, y_test_predicted))       # 0.98
    # print("Clf report: \n", classification_report(y_test, y_test_predicted))
    # print("Confusion matrix: \n", confusion_matrix(y_test, y_test_predicted))

    # # Train and test MLP model on tf-idf dataset
    # mlp_model = MLPClassifier(hidden_layer_sizes=(10, 4))
    # mlp_model.fit(X_train, y_train)
    # y_test_predicted = mlp_model.predict(X_test)
    # print("Accuracy score: \n", accuracy_score(y_test, y_test_predicted))       # 0.93
    # print("Clf report: \n", classification_report(y_test, y_test_predicted))
    # print("Confusion matrix: \n", confusion_matrix(y_test, y_test_predicted))

